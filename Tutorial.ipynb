{"cells":[{"cell_type":"markdown","metadata":{"id":"xjZ59jjnEe2U"},"source":["Resources Used\n","- wget.download('https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/_downloads/da4babe668a8afb093cc7776d7e630f3/generate_tfrecord.py')\n","- Setup https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html"]},{"cell_type":"markdown","metadata":{"id":"hUmSo6F4Ee2W"},"source":["# 0. Setup Paths"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"VrDqmY-cEe2W"},"outputs":[],"source":["WORKSPACE_PATH = 'Tensorflow/workspace'\n","SCRIPTS_PATH = 'Tensorflow/scripts'\n","APIMODEL_PATH = 'Tensorflow/models'\n","ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\n","IMAGE_PATH = WORKSPACE_PATH+'/images'\n","MODEL_PATH = WORKSPACE_PATH+'/models'\n","PRETRAINED_MODEL_PATH = WORKSPACE_PATH+'/pre-trained-models'\n","CONFIG_PATH = MODEL_PATH+'/my_ssd_mobnet/pipeline.config'\n","CHECKPOINT_PATH = MODEL_PATH+'/my_ssd_mobnet/'"]},{"cell_type":"markdown","metadata":{"id":"_H4gPdWuEe2X"},"source":["# 1. Create Label Map"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"Uu4MbMDKEe2X"},"outputs":[],"source":["labels = [\n","    {'name':'Hello', 'id':1}, \n","    {'name':'Yes', 'id':2},\n","    {'name':'No', 'id':3},\n","    {'name':'Thanks', 'id':4},\n","    {'name':'I Love You', 'id':5},\n","]\n","\n","with open(ANNOTATION_PATH + '\\label_map.pbtxt', 'w') as f:\n","    for label in labels:\n","        f.write('item { \\n')\n","        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n","        f.write('\\tid:{}\\n'.format(label['id']))\n","        f.write('}\\n')"]},{"cell_type":"markdown","metadata":{"id":"XTNOjz4EEe2X"},"source":["# 2. Create TF records"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"wKf8MyWpEe2Y","outputId":"d29a0044-4a9f-4365-827e-70c35bd7063c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"Tensorflow/scripts/generate_tfrecord.py\", line 168, in <module>\n","    tf.app.run()\n","  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\n","    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n","  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\absl\\app.py\", line 303, in run\n","    _run_main(main, args)\n","  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\n","    sys.exit(main(argv))\n","  File \"Tensorflow/scripts/generate_tfrecord.py\", line 158, in main\n","    tf_example = create_tf_example(group, path)\n","  File \"Tensorflow/scripts/generate_tfrecord.py\", line 132, in create_tf_example\n","    classes.append(class_text_to_int(row['class']))\n","  File \"Tensorflow/scripts/generate_tfrecord.py\", line 101, in class_text_to_int\n","    return label_map_dict[row_label]\n","KeyError: 'Good Bye'\n","Traceback (most recent call last):\n","  File \"Tensorflow/scripts/generate_tfrecord.py\", line 168, in <module>\n","    tf.app.run()\n","  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\n","    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n","  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\absl\\app.py\", line 303, in run\n","    _run_main(main, args)\n","  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\n","    sys.exit(main(argv))\n","  File \"Tensorflow/scripts/generate_tfrecord.py\", line 158, in main\n","    tf_example = create_tf_example(group, path)\n","  File \"Tensorflow/scripts/generate_tfrecord.py\", line 132, in create_tf_example\n","    classes.append(class_text_to_int(row['class']))\n","  File \"Tensorflow/scripts/generate_tfrecord.py\", line 101, in class_text_to_int\n","    return label_map_dict[row_label]\n","KeyError: 'Good Bye'\n"]}],"source":["!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x {IMAGE_PATH + '/train'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/train.record'}\n","!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x{IMAGE_PATH + '/test'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/test.record'}"]},{"cell_type":"markdown","metadata":{"id":"sgqyrv_mEe2Y"},"source":["# 3. Download TF Models Pretrained Models from Tensorflow Model Zoo"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"uwa2numpEe2Y","outputId":"982be4d1-f46f-455f-eaa8-e8be98bce3de"},"outputs":[{"name":"stderr","output_type":"stream","text":["fatal: destination path 'models' already exists and is not an empty directory.\n"]}],"source":["!cd Tensorflow && git clone https://github.com/tensorflow/models"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"RZDrT8VQEe2Z"},"outputs":[],"source":["#wget.download('http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz')\n","#!mv ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz {PRETRAINED_MODEL_PATH}\n","#!cd {PRETRAINED_MODEL_PATH} && tar -zxvf ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz"]},{"cell_type":"markdown","metadata":{"id":"OXZxyj3WEe2Z"},"source":["# 4. Copy Model Config to Training Folder"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"3YKchIfJEe2Z"},"outputs":[],"source":["CUSTOM_MODEL_NAME = 'my_ssd_mobnet' "]},{"cell_type":"code","execution_count":38,"metadata":{"id":"vJG7ilxkEe2Z","outputId":"d31f6254-f169-4592-a8a2-f62e5e24a459"},"outputs":[{"name":"stderr","output_type":"stream","text":["A subdirectory or file Tensorflow\\workspace\\models\\my_ssd_mobnet already exists.\n"]},{"name":"stdout","output_type":"stream","text":["The syntax of the command is incorrect.\n"]}],"source":["!mkdir {'Tensorflow\\workspace\\models\\\\'+CUSTOM_MODEL_NAME}\n","!copy {PRETRAINED_MODEL_PATH+'\\\\ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\\pipeline.config'} {MODEL_PATH+'\\\\'+CUSTOM_MODEL_NAME}"]},{"cell_type":"markdown","metadata":{"id":"rOqLuFDcEe2Z"},"source":["# 5. Update Config For Transfer Learning"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"EZPvJ_aoEe2Z"},"outputs":[],"source":["import tensorflow as tf\n","from object_detection.utils import config_util\n","from object_detection.protos import pipeline_pb2\n","from google.protobuf import text_format"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"2TPZv5CwEe2Z"},"outputs":[],"source":["CONFIG_PATH = MODEL_PATH+'/'+CUSTOM_MODEL_NAME+'/pipeline.config'"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"Yy1ItkIdEe2a"},"outputs":[],"source":["config = config_util.get_configs_from_pipeline_file(CONFIG_PATH)"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"IO0k2xEEEe2a","outputId":"f76e8839-5a02-4b26-959b-72d760533291"},"outputs":[{"data":{"text/plain":["{'model': ssd {\n","   num_classes: 5\n","   image_resizer {\n","     fixed_shape_resizer {\n","       height: 320\n","       width: 320\n","     }\n","   }\n","   feature_extractor {\n","     type: \"ssd_mobilenet_v2_fpn_keras\"\n","     depth_multiplier: 1.0\n","     min_depth: 16\n","     conv_hyperparams {\n","       regularizer {\n","         l2_regularizer {\n","           weight: 3.9999998989515007e-05\n","         }\n","       }\n","       initializer {\n","         random_normal_initializer {\n","           mean: 0.0\n","           stddev: 0.009999999776482582\n","         }\n","       }\n","       activation: RELU_6\n","       batch_norm {\n","         decay: 0.996999979019165\n","         scale: true\n","         epsilon: 0.0010000000474974513\n","       }\n","     }\n","     use_depthwise: true\n","     override_base_feature_extractor_hyperparams: true\n","     fpn {\n","       min_level: 3\n","       max_level: 7\n","       additional_layer_depth: 128\n","     }\n","   }\n","   box_coder {\n","     faster_rcnn_box_coder {\n","       y_scale: 10.0\n","       x_scale: 10.0\n","       height_scale: 5.0\n","       width_scale: 5.0\n","     }\n","   }\n","   matcher {\n","     argmax_matcher {\n","       matched_threshold: 0.5\n","       unmatched_threshold: 0.5\n","       ignore_thresholds: false\n","       negatives_lower_than_unmatched: true\n","       force_match_for_each_row: true\n","       use_matmul_gather: true\n","     }\n","   }\n","   similarity_calculator {\n","     iou_similarity {\n","     }\n","   }\n","   box_predictor {\n","     weight_shared_convolutional_box_predictor {\n","       conv_hyperparams {\n","         regularizer {\n","           l2_regularizer {\n","             weight: 3.9999998989515007e-05\n","           }\n","         }\n","         initializer {\n","           random_normal_initializer {\n","             mean: 0.0\n","             stddev: 0.009999999776482582\n","           }\n","         }\n","         activation: RELU_6\n","         batch_norm {\n","           decay: 0.996999979019165\n","           scale: true\n","           epsilon: 0.0010000000474974513\n","         }\n","       }\n","       depth: 128\n","       num_layers_before_predictor: 4\n","       kernel_size: 3\n","       class_prediction_bias_init: -4.599999904632568\n","       share_prediction_tower: true\n","       use_depthwise: true\n","     }\n","   }\n","   anchor_generator {\n","     multiscale_anchor_generator {\n","       min_level: 3\n","       max_level: 7\n","       anchor_scale: 4.0\n","       aspect_ratios: 1.0\n","       aspect_ratios: 2.0\n","       aspect_ratios: 0.5\n","       scales_per_octave: 2\n","     }\n","   }\n","   post_processing {\n","     batch_non_max_suppression {\n","       score_threshold: 9.99999993922529e-09\n","       iou_threshold: 0.6000000238418579\n","       max_detections_per_class: 100\n","       max_total_detections: 100\n","       use_static_shapes: false\n","     }\n","     score_converter: SIGMOID\n","   }\n","   normalize_loss_by_num_matches: true\n","   loss {\n","     localization_loss {\n","       weighted_smooth_l1 {\n","       }\n","     }\n","     classification_loss {\n","       weighted_sigmoid_focal {\n","         gamma: 2.0\n","         alpha: 0.25\n","       }\n","     }\n","     classification_weight: 1.0\n","     localization_weight: 1.0\n","   }\n","   encode_background_as_zeros: true\n","   normalize_loc_loss_by_codesize: true\n","   inplace_batchnorm_update: true\n","   freeze_batchnorm: false\n"," },\n"," 'train_config': batch_size: 4\n"," data_augmentation_options {\n","   random_horizontal_flip {\n","   }\n"," }\n"," data_augmentation_options {\n","   random_crop_image {\n","     min_object_covered: 0.0\n","     min_aspect_ratio: 0.75\n","     max_aspect_ratio: 3.0\n","     min_area: 0.75\n","     max_area: 1.0\n","     overlap_thresh: 0.0\n","   }\n"," }\n"," sync_replicas: true\n"," optimizer {\n","   momentum_optimizer {\n","     learning_rate {\n","       cosine_decay_learning_rate {\n","         learning_rate_base: 0.07999999821186066\n","         total_steps: 50000\n","         warmup_learning_rate: 0.026666000485420227\n","         warmup_steps: 1000\n","       }\n","     }\n","     momentum_optimizer_value: 0.8999999761581421\n","   }\n","   use_moving_average: false\n"," }\n"," fine_tune_checkpoint: \"Tensorflow/workspace/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n"," num_steps: 50000\n"," startup_delay_steps: 0.0\n"," replicas_to_aggregate: 8\n"," max_number_of_boxes: 100\n"," unpad_groundtruth_tensors: false\n"," fine_tune_checkpoint_type: \"detection\"\n"," fine_tune_checkpoint_version: V2,\n"," 'train_input_config': label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n"," tf_record_input_reader {\n","   input_path: \"Tensorflow/workspace/annotations/train.record\"\n"," },\n"," 'eval_config': metrics_set: \"coco_detection_metrics\"\n"," use_moving_averages: false,\n"," 'eval_input_configs': [label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n"," shuffle: false\n"," num_epochs: 1\n"," tf_record_input_reader {\n","   input_path: \"Tensorflow/workspace/annotations/test.record\"\n"," }\n"," ],\n"," 'eval_input_config': label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n"," shuffle: false\n"," num_epochs: 1\n"," tf_record_input_reader {\n","   input_path: \"Tensorflow/workspace/annotations/test.record\"\n"," }}"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["config"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"I-XMvTEQEe2a"},"outputs":[],"source":["pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n","with tf.io.gfile.GFile(CONFIG_PATH, \"r\") as f:                                                                                                                                                                                                                     \n","    proto_str = f.read()                                                                                                                                                                                                                                          \n","    text_format.Merge(proto_str, pipeline_config)  "]},{"cell_type":"code","execution_count":44,"metadata":{"id":"zwMI4y4jEe2a"},"outputs":[],"source":["pipeline_config.model.ssd.num_classes = 5\n","pipeline_config.train_config.batch_size = 4\n","pipeline_config.train_config.fine_tune_checkpoint = PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0'\n","pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n","pipeline_config.train_input_reader.label_map_path= ANNOTATION_PATH + '/label_map.pbtxt'\n","pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/train.record']\n","pipeline_config.eval_input_reader[0].label_map_path = ANNOTATION_PATH + '/label_map.pbtxt'\n","pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/test.record']"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"FD1OJDP5Ee2a"},"outputs":[],"source":["config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n","with tf.io.gfile.GFile(CONFIG_PATH, \"wb\") as f:                                                                                                                                                                                                                     \n","    f.write(config_text)   "]},{"cell_type":"markdown","metadata":{"id":"LM1rFw99Ee2a"},"source":["# 6. Train the model"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"Oy2HPUSVEe2a","outputId":"830c0223-e1d2-4b22-ee45-51527793effe"},"outputs":[{"name":"stdout","output_type":"stream","text":["python Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=Tensorflow/workspace/models/my_ssd_mobnet --pipeline_config_path=Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config --num_train_steps=10000\n"]}],"source":["print(\"\"\"python {}/research/object_detection/model_main_tf2.py --model_dir={}/{} --pipeline_config_path={}/{}/pipeline.config --num_train_steps=10000\"\"\".format(APIMODEL_PATH, MODEL_PATH,CUSTOM_MODEL_NAME,MODEL_PATH,CUSTOM_MODEL_NAME))"]},{"cell_type":"markdown","metadata":{"id":"FIaYp9mqEe2b"},"source":["# 7. Load Train Model From Checkpoint"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"ddimdYOVEe2b"},"outputs":[],"source":["import os\n","from object_detection.utils import label_map_util\n","from object_detection.utils import visualization_utils as viz_utils\n","from object_detection.builders import model_builder"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"rTuKn5BbEe2b"},"outputs":[],"source":["# Load pipeline config and build a detection model\n","configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n","detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n","\n","# Restore checkpoint\n","ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n","ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-13')).expect_partial()\n","\n","@tf.function\n","def detect_fn(image):\n","    image, shapes = detection_model.preprocess(image)\n","    prediction_dict = detection_model.predict(image, shapes)\n","    detections = detection_model.postprocess(prediction_dict, shapes)\n","    return detections"]},{"cell_type":"markdown","metadata":{"id":"UJPbww-YEe2b"},"source":["# 8. Detect in Real-Time"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"oK5OqusJEe2b"},"outputs":[],"source":["import cv2 \n","import numpy as np\n","import torch\n","from torch_geometric.data import Data\n","from torchvision import ops as box_utils"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"ixXltHf3Ee2b"},"outputs":[],"source":["category_index = label_map_util.create_category_index_from_labelmap(ANNOTATION_PATH+'/label_map.pbtxt')"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"sellKCoEEe2b"},"outputs":[{"ename":"NameError","evalue":"name 'cap' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-25-e0a928c10214>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mNameError\u001b[0m: name 'cap' is not defined"]}],"source":["cap.release()"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"R1I5antSEe2b"},"outputs":[],"source":["# Setup capture\n","cap = cv2.VideoCapture(0)\n","width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"sBLBx_4UEe2c","outputId":"a8903bf5-86c7-4ffc-dce9-bee96334c963"},"outputs":[{"name":"stderr","output_type":"stream","text":["Exception ignored in: <bound method ScopedTFGraph.__del__ of <tensorflow.python.framework.c_api_util.ScopedTFGraph object at 0x0000015268D66208>>\n","Traceback (most recent call last):\n","  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\c_api_util.py\", line 58, in __del__\n","    self.deleter(self.graph)\n","AttributeError: deleter\n"]}],"source":["while True: \n","    ret, frame = cap.read()\n","    image_np = np.array(frame)\n","    \n","    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n","    detections = detect_fn(input_tensor)\n","    \n","    num_detections = int(detections.pop('num_detections'))\n","    detections = {key: value[0, :num_detections].numpy()\n","                  for key, value in detections.items()}\n","    detections['num_detections'] = num_detections\n","\n","    # detection_classes should be ints.\n","    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n","\n","    label_id_offset = 1\n","    image_np_with_detections = image_np.copy()\n","\n","    viz_utils.visualize_boxes_and_labels_on_image_array(\n","                image_np_with_detections,\n","                detections['detection_boxes'],\n","                detections['detection_classes']+label_id_offset,\n","                detections['detection_scores'],\n","                category_index,\n","                use_normalized_coordinates=True,\n","                max_boxes_to_draw=5,\n","                min_score_thresh=.5,\n","                agnostic_mode=False)\n","\n","    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n","    \n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        cap.release()\n","        break"]},{"cell_type":"markdown","metadata":{"id":"tlDgzmUeOOcD"},"source":["COBA YANG INI"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"r8dr2T7NLXvR"},"outputs":[{"ename":"ValueError","evalue":"in user code:\n\n    <ipython-input-41-b0a77f174d1b>:11 detect_fn  *\n        image, shapes = detection_model.preprocess(image)\n    c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\object_detection\\meta_architectures\\ssd_meta_arch.py:485 preprocess  *\n        normalized_inputs, self._image_resizer_fn)\n    c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\object_detection\\utils\\shape_utils.py:492 resize_images_and_return_shapes  *\n        outputs = static_or_dynamic_map_fn(\n    c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\object_detection\\utils\\shape_utils.py:246 static_or_dynamic_map_fn  *\n        outputs = [fn(arg) for arg in tf.unstack(elems)]\n    c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\object_detection\\core\\preprocessor.py:3330 resize_image  *\n        new_image = tf.image.resize_images(\n    c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1544 resize_images\n        skip_resize_if_same=True)\n    c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1396 _resize_images_common\n        raise ValueError('\\'images\\' must have either 3 or 4 dimensions.')\n\n    ValueError: 'images' must have either 3 or 4 dimensions.\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m<ipython-input-53-d320195ece7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mssd_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnum_detections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mssd_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'num_detections'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m-> 3038\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n","\u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3463\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3308\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-41-b0a77f174d1b>:11 detect_fn  *\n        image, shapes = detection_model.preprocess(image)\n    c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\object_detection\\meta_architectures\\ssd_meta_arch.py:485 preprocess  *\n        normalized_inputs, self._image_resizer_fn)\n    c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\object_detection\\utils\\shape_utils.py:492 resize_images_and_return_shapes  *\n        outputs = static_or_dynamic_map_fn(\n    c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\object_detection\\utils\\shape_utils.py:246 static_or_dynamic_map_fn  *\n        outputs = [fn(arg) for arg in tf.unstack(elems)]\n    c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\object_detection\\core\\preprocessor.py:3330 resize_image  *\n        new_image = tf.image.resize_images(\n    c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1544 resize_images\n        skip_resize_if_same=True)\n    c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1396 _resize_images_common\n        raise ValueError('\\'images\\' must have either 3 or 4 dimensions.')\n\n    ValueError: 'images' must have either 3 or 4 dimensions.\n"]}],"source":["while True: \n","    ret, frame = cap.read()\n","    image_np = np.array(frame)\n","    \n","    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n","    ssd_features = detect_fn(input_tensor)\n","    \n","    num_detections = int(ssd_features.pop('num_detections'))\n","    ssd_features = {key: value[0, :num_detections].numpy()\n","                  for key, value in ssd_features.items()}\n","    ssd_features['num_detections'] = num_detections\n","\n","    # detection_classes should be ints.\n","    ssd_features['detection_classes'] = ssd_features['detection_classes'].astype(np.int64)\n","\n","    detections = detect_fn(input_tensor)\n","    valid_indices = np.where(detections['detection_scores'] > score_threshold)[0]\n","    valid_boxes = detections['detection_boxes'][valid_indices]\n","    iou = box_utils.bbox_overlaps(valid_boxes, valid_boxes)\n","    edge_mask = iou > iou_threshold\n","    edge_indices = np.argwhere(edge_mask)\n","    edge_index = torch.from_numpy(edge_indices)\n","\n","    # Create graph object\n","    graph = Data(x=ssd_features, edge_index=edge_index)\n","    \n","    # Pass the graph data to your GCN model\n","    graph_features = gcn(graph)\n","    \n","    # Concatenate the SSD features with the GCN features\n","    concatenated_features = torch.cat((ssd_features, graph_features), axis=-1)\n","\n","    # Create a fully connected layer to classify the object\n","    fc = torch.nn.Linear(concatenated_features.shape[-1], num_classes)\n","\n","    # Pass the concatenated features through the fully connected layer\n","    logits = fc(concatenated_features)\n","\n","    # Apply softmax on the logits to get the class probabilities\n","    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n","\n","    # Get the class with the highest probability\n","    class_id = torch.argmax(probabilities)\n","\n","    # Draw the predicted class on the image\n","    cv2.putText(image_np, class_names[class_id], (10, 30),\n","        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n","\n","    # Show the image\n","    cv2.imshow('object detection',  cv2.resize(image_np, (800, 600)))\n","\n","    # Exit if the user presses 'q'\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        cap.release()\n","        cv2.destroyAllWindows()\n","        break    "]},{"cell_type":"code","execution_count":33,"metadata":{"id":"CYSy7ItVMQMT"},"outputs":[{"ename":"SyntaxError","evalue":"'break' outside loop (<ipython-input-33-941b7d22687c>, line 46)","output_type":"error","traceback":["\u001b[1;36m  File \u001b[1;32m\"<ipython-input-33-941b7d22687c>\"\u001b[1;36m, line \u001b[1;32m46\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"]}],"source":["    ssd_features = detect_fn(input_tensor)\n","    \n","    valid_indices = np.where(detections['detection_scores'] > score_threshold)[0]\n","    valid_boxes = detections['detection_boxes'][valid_indices]\n","    iou = box_utils.bbox_overlaps(valid_boxes, valid_boxes)\n","    edge_mask = iou > iou_threshold\n","    edge_indices = np.argwhere(edge_mask)\n","    edge_index = torch.from_numpy(edge_indices)\n","\n","    \n","    # Create graph object\n","    graph = Data(x=ssd_features, edge_index=edge_index)\n","    \n","    # Pass the graph data to your GCN model\n","    graph_features = gcn(graph)\n","    \n","    # Concatenate the SSD features with the GCN features\n","    concatenated_features = torch.cat((ssd_features, graph_features), axis=-1)\n","\n","    # Create a fully connected layer to classify the object\n","    fc = torch.nn.Linear(concatenated_features.shape[-1], num_classes)\n","\n","    # Pass the concatenated features through the fully connected layer\n","    logits = fc(concatenated_features)\n","\n","    # Apply softmax on the logits to get the class probabilities\n","    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n","\n","    # Get the class with the highest probability\n","    class_id = torch.argmax(probabilities)\n","\n","    # Draw the predicted class on the image\n","    cv2.putText(image_np, class_names[class_id], (10, 30),\n","        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n","\n","    # Show the image\n","    cv2.imshow('Aplikasi Deteksi Bahasa Isyarat',  cv2.resize(image_np, (800, 600)))\n","\n","    # Exit if the user presses 'q'\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        cap.release()\n","        cv2.destroyAllWindows()\n","        break    "]},{"cell_type":"code","execution_count":37,"metadata":{"id":"c-aYxJxWEe2c"},"outputs":[{"ename":"NameError","evalue":"name 'detect_fn' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-37-3f78138d26f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdetections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mNameError\u001b[0m: name 'detect_fn' is not defined"]}],"source":["detections = detect_fn(input_tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EzYDNFbhEe2c"},"outputs":[],"source":["from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqYbIVhlEe2c"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"vscode":{"interpreter":{"hash":"15587d10115f62644f5173f02f872b68f2ff997d65d238218ed66f873184aa97"}}},"nbformat":4,"nbformat_minor":0}
